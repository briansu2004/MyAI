# AI & Myself

- [Sponsor My AI Projects](#sponsor-my-ai-projects)
  - [An AI-powered basketball foul judgment system](#an-ai-powered-basketball-foul-judgment-system)
    - [1. **Smart Basketball Court System**](#1-smart-basketball-court-system)
    - [2. **AI Algorithm Analysis**](#2-ai-algorithm-analysis)
    - [3. **Real-Time Feedback System**](#3-real-time-feedback-system)
    - [4. **Feasibility and Challenges**](#4-feasibility-and-challenges)
  - [Predicting and Addressing Criminal Activities](#predicting-and-addressing-criminal-activities)
- [My AI Skills](#my-ai-skills)
- [My AI Work Projects](#my-ai-work-projects)
  - [Project 6](#project-6)
  - [Project 5](#project-5)
  - [Project 4](#project-4)
  - [Project 3](#project-3)
  - [Project 2](#project-2)
  - [Project 1](#project-1)
- [My AI Platforms](#my-ai-platforms)
  - [Amazon Web Services (AWS)](#amazon-web-services-aws)
  - [Google Cloud Platform (GCP)](#google-cloud-platform-gcp)
  - [Microsoft Azure](#microsoft-azure)
  - [IBM Watson](#ibm-watson)

<!-- ## My AI WIP Proposals / Projects -->

## Sponsor My AI Projects

### An AI-powered basketball foul judgment system

To ensure fair foul judgments in basketball games with friends, we can design a system that leverages **sensor technology and AI algorithms**. Here's a feasible invention idea:

#### 1. **Smart Basketball Court System**

Integrate sensors and AI to record match data in real-time and automatically determine fouls.

##### (1) **Wearable Devices**

- **Smart Wristbands or Protective Gear**: Each player wears lightweight smart devices (e.g., wristbands, knee pads) equipped with accelerometers, gyroscopes, and pressure sensors to monitor body movements, collision intensity, and direction.
- **Shoe Sensors**: Embedded pressure sensors to detect foot movement, stepping out of bounds, or illegal steps.

##### (2) **Smart Basketball**

- Built-in sensors: Detect the ball's speed, position, and contact details.
- Integration with wearables: Monitor illegal contacts (e.g., hand-checking or slapping the ball).

##### (3) **Court Sensors**

- Pressure sensors under the floor: Track player movements, positions, and foot placement in real-time.
- Camera system: Capture player actions from multiple angles and use AI algorithms to identify fouls (e.g., pushing, grabbing).

#### 2. **AI Algorithm Analysis**

- **Action Recognition**: Use computer vision and motion capture algorithms to analyze player movements against rules.
- **Collision Intensity Detection**: Determine if the intensity of contact exceeds foul thresholds using sensor data.
- **Rule Matching**: Compare sensor data with a built-in rule database to decide on fouls in real-time.

#### 3. **Real-Time Feedback System**

- Penalty Notifications: Display foul decisions via screens on the court or through players' devices.
- Replay Support: Provide slow-motion replays to explain foul calls, helping both sides understand the rationale.

#### 4. **Feasibility and Challenges**

- **Advantages**:
  - More objective and fair decisions.
  - Reduce disputes caused by foul controversies.
  - Add a sense of technology to casual games.
- **Challenges**:
  - Equipment cost: Making smart devices affordable for widespread use.
  - Data accuracy: The AI algorithm requires sufficient training data to minimize errors.
  - Rule adaptation: Setting reasonable penalty thresholds to avoid being overly strict.

Such a system is not only suitable for casual games with friends but also for amateur or semi-professional leagues.

<!-- If you're looking to gather investments for this idea, you can emphasize the potential market for integrating technology into sports and focus on:

1. **Prototype Development**: Showcase a small-scale working model to attract investors.
2. **Target Audience**: Highlight applications for amateur games, training camps, or sports academies.
3. **Scalability**: Mention how it can expand to professional leagues in the future. -->

### Predicting and Addressing Criminal Activities

I need some sponsors!

Here is my WIP project:

- Gather the data from the history for all criminal events in my city
- Take the live feed from different resources (e.g. police reports, call centers, governments, witnesses etc.)
- Do the data cleansing and remove outliners
- Split the data set for training and test data
- Choose the best ML algorithm and use it to analyze the data
- Predict the highest freqenent days/hours for all particular zones
- Notify the police stations and recommend them to prepare more on the crime peek days/hours

Here's a high-level overview of how I will proceed:

1. **Data Gathering**: Collect historical data on criminal events in our city from relevant sources such as police records, crime databases, news reports, and government statistics. Additionally, set up mechanisms to gather live data from sources like police reports, emergency call centers, government agencies, and eyewitness accounts.

2. **Data Cleansing and Preprocessing**: Cleanse the collected data to remove inconsistencies, errors, and outliers. This may involve tasks such as handling missing values, standardizing data formats, and identifying and removing erroneous entries.

3. **Data Splitting**: Split the cleaned dataset into training and testing subsets. The training dataset will be used to train our machine learning model, while the testing dataset will be used to evaluate its performance.

4. **Model Selection and Training**: Choose the most appropriate machine learning algorithm for our problem. This could involve experimenting with different algorithms such as decision trees, random forests, support vector machines, or neural networks. Train the selected model using the training dataset.

5. **Model Evaluation**: Evaluate the performance of our trained model using the testing dataset. Common evaluation metrics for classification tasks include accuracy, precision, recall, and F1 score. Choose the evaluation metrics that are most relevant to our project's objectives.

6. **Prediction**: Once our model is trained and evaluated, use it to make predictions on new data. In this case, use the trained model to predict the highest frequency days and hours for criminal events in particular zones of our city.

7. **Notification and Recommendation**: Develop a mechanism to notify relevant authorities, such as police stations, about the predicted high-frequency days and hours for criminal activity in specific zones. Additionally, provide recommendations on how they can prepare and allocate resources accordingly.

8. **Deployment and Monitoring**: Deploy our model and notification system in a production environment. Continuously monitor the system's performance and update the model as needed with new data and insights.

9. **Feedback Loop**: Establish a feedback loop to collect data on the effectiveness of our predictions and recommendations. Use this feedback to refine and improve our model over time.

10. **Documentation and Maintenance**: Document the entire project, including data sources, preprocessing steps, model selection, training, and deployment processes. Regularly maintain and update our system to ensure its effectiveness and relevance over time.

<!-- By following these steps, I can successfully complete and implement our AI ML DevOps project for predicting and addressing criminal activity in our city. -->

## My AI Skills

LLM, Prompting, Markdown Prompts Framework; Machine Learning Algorithms, linear regression, logistic regression, decision trees, random forests, support vector machines, k-nearest neighbors, naive Bayes; Data Preprocessing, Model Evaluation and Validation, Feature Selection and Engineering, Model Deployment, TensorFlow; Data Science, Python (NumPy, Pandas, Scikit-learn, TensorFlow, Keras), R, Jupyter Notebooks; Data visualization libraries, Matplotlib, Seaborn, and Plotly; NLP; Computer Vision, PyTorch; IBM Watson, Azure AI platform, AWS AI platform, GCP AI platform; LangChain

<!-- Machine Learning Algorithms, linear regression, logistic regression, decision trees, random forests, support vector machines, k-nearest neighbors, naive Bayes; Data Preprocessing, Model Evaluation and Validation, Feature Selection and Engineering, Model Deployment, TensorFlow; Data Science, Python (NumPy, Pandas, Scikit-learn, TensorFlow, Keras), R, Jupyter Notebooks; Data visualization libraries, Matplotlib, Seaborn, and Plotly; NLP; Computer Vision, PyTorch; IBM Watson, Azure AI platform, AWS AI platform, GCP AI platform -->

1. **Machine Learning Algorithms**: linear regression, logistic regression, decision trees, random forests, support vector machines, k-nearest neighbors, naive Bayes, gradient boosting, and neural networks (including deep learning architectures such as CNNs, RNNs, and LSTMs).

2. **Data Preprocessing**: feature scaling, feature engineering, dimensionality reduction (PCA, LDA), and handling missing data.

3. **Model Evaluation and Validation**: evaluating and validating machine learning models using techniques like cross-validation, hyperparameter tuning, and metrics such as accuracy, precision, recall, F1-score, ROC-AUC, and mean squared error (MSE).

4. **Feature Selection and Engineering**: selecting relevant features, creating new features, and performing feature engineering tasks like one-hot encoding, label encoding, and handling categorical variables.

5. **Model Deployment**: deploying machine learning models into production environments using frameworks like TensorFlow Serving, Flask, Django, or cloud-based deployment platforms like AWS SageMaker or Google Cloud AI Platform.

6. **Data Science Tools**: Python (NumPy, Pandas, Scikit-learn, TensorFlow, Keras), R, Jupyter Notebooks, and data visualization libraries like Matplotlib, Seaborn, and Plotly.

7. **Data Analysis**: exploratory data analysis (EDA) and draw insights from data using statistical methods, visualization techniques, and domain knowledge.

8. **Natural Language Processing (NLP)**: text classification, sentiment analysis, named entity recognition, topic modeling, and building chatbots or virtual assistants.

9. **Computer Vision**: image classification, object detection, image segmentation, and building models with frameworks like OpenCV, TensorFlow, or PyTorch.

10. **Reinforcement Learning**: Q-learning, deep Q-networks (DQN), and policy gradients.

## My AI Work Projects

### Project 6

<!-- Hebei Lizhong Group 立中集团 -->

Designed and delivered a privacy-preserving, AI-powered chatbot solution as a proof of concept (POC) for a client in a private industry domain.

- The POC aimed to develop a business-facing chatbot capable of acting as a virtual sales or customer representative by integrating private client data with LLM capabilities.
- Deployed Ollama in the client's local environment and evaluated multiple LLMs, selecting Gemma for its performance and compatibility for the demo.
- Compared and benchmarked several vector database solutions - FAISS, Pinecone, Qdrant, and Weaviate - ultimately choosing Qdrant for its balance of flexibility, performance, and ease of deployment.
- Built a RAG (Retrieval-Augmented Generation) pipeline using Python to handle embedding, indexing, document retrieval, and response generation.
- Integrated Model Context Protocol (MCP) to modularize and manage business-specific context for LLM prompts, enabling traceable, reusable, and versioned knowledge injection.
- Developed backend APIs using FastAPI, enabling seamless communication between the LLM, vector DB, and frontend.
- Designed and implemented user interfaces using React and Vue, enhancing user interaction and delivering a modern conversational experience.
- The chatbot successfully blended proprietary business data with general language model outputs, demonstrating strong performance and real-world applicability in client-specific use cases.

### Project 5

<!-- Hebei Lizhong Group 立中集团 -->

Developed and optimized a multimodal AI solution based on the latest MiniCPM-V series models for a client-facing proof of concept.

- The project focused on MiniCPM-Llama3-V 2.5 and MiniCPM-V 2.0, implementing real-world applications in OCR and multilingual dialogue systems using advanced MLLMs (Multimodal Large Language Models).
- Built interactive web-based QA systems using Gradio and Streamlit, enhancing user experience through responsive interfaces and real-time inference integration.
- Applied quantization techniques (8-bit and 4-bit) using the llama.cpp framework to optimize model performance under resource constraints.
- Explored vLLM-based inference, including source code analysis to understand internal model execution and streamline deployment.
- Implemented both full-parameter fine-tuning and LoRA-based fine-tuning using Hugging Face Transformers with DeepSpeed, enabling scalable model customization on proprietary datasets.
- Configured advanced training pipelines with customized datasets to address client-specific tasks, improving adaptability and task accuracy.
- Performed model performance evaluation and optimization, including experiment tracking, output quality assessment, and latency reduction.
- Researched and integrated the latest techniques in multimodal learning, aligning the solution with current industry trends and research directions.
- Delivered a fully functional prototype with detailed technical documentation and deployment instructions for client adoption and future scalability.

### Project 4

<!-- Hebei Lizhong Group 立中集团 -->

Designed and implemented a localized, privacy-focused AI solution as a proof of concept for a client.

- The POC involved deploying and managing large language models (LLMs) such as Gemma and Ollama on the client's Windows and Mac environments using Docker.
- Focused on data privacy and on-premise control, the solution included full environment setup and container orchestration.
- Integrated advanced AI tools like openWebUI and anythingLLM to develop a customized AI assistant and secure knowledge base.
- Configured environment variables, managed local model files, and optimized inference performance with precise control over system resources.
- Built an interactive user interface with openWebUI to enhance usability and AI interaction.
- Constructed a local knowledge repository using anythingLLM for intelligent document management and semantic retrieval.
- Bridged the gap between cutting-edge AI capabilities and real-world deployment through a hands-on, secure architecture.
- Provided detailed guidance to the client for operating, maintaining, and expanding the AI system autonomously.
- Empowered the client to adopt AI innovations internally while maintaining full ownership of their data and infrastructure.

<!-- Machine Learning, Data Science etc. -->

### Project 3

<!-- Built a knowledge-based ontology graph database application for a federal client

- Converted relational data model to Graph data model and imported to Neo4j.
- Used Python NLP libraries and Azure Cognitive Services APIs to analyse data, extracted entities, key phrases, and categories, then do sentiment analysis with data science algorithms in Jupyter Notebook.
- Visualized the ontology knowledge graphs with Neo4j Bloom and search phrases. -->

<!-- A confidential federal client -->

Developed a sophisticated knowledge-based ontology graph database application tailored for a federal client's needs.

- Seamlessly transitioned from a traditional relational data model to a dynamic Graph data model, leveraging the robust capabilities of Neo4j for enhanced data representation and querying efficiency.
- Implemented a comprehensive Natural Language Processing (NLP) pipeline utilizing cutting-edge Python libraries and Azure Cognitive Services APIs. This pipeline adeptly analyzed extensive datasets, extracting pertinent entities, identifying key phrases, categorizing content, and performing sentiment analysis.
- Leveraged advanced data science algorithms within Jupyter Notebook to derive meaningful insights from the analyzed data, empowering decision-makers with actionable intelligence.
- Utilized Neo4j Bloom to visualize intricate ontology knowledge graphs, facilitating intuitive exploration and understanding of complex relationships within the data.
- Expanded the scope of data acquisition by integrating additional feeds from diverse social media platforms. Leveraged the updated data to re-run the NLP pipeline, ensuring that insights remained current and reflective of evolving trends and sentiments across various digital channels.

### Project 2

<!-- A confidential government client -->

<!-- Built a Python AI POC for a government client to predict how much percent the issued parking tickets can be paid

- Collected the data from various sources
- Filtered out the outliers
- Split for training and testing
- Used a few algorithms and chose the best model which has the best Area Under the ROC Curve. -->

Developed a Python-based AI project for a government client aimed at predicting the likelihood of payment for issued parking tickets.

- Gathered diverse datasets from multiple sources to ensure comprehensive coverage and accuracy in training the predictive model.
- Employed robust data preprocessing techniques to filter out outliers and ensure the integrity of the dataset, enhancing the reliability of subsequent analyses.
- Split the dataset into distinct training and testing sets to enable the evaluation of model performance on unseen data, mitigating against overfitting and ensuring generalizability.
- Leveraged a variety of machine learning algorithms to train predictive models, rigorously evaluating their performance against established metrics.
- Utilized Area Under the ROC Curve (AUC-ROC) as a primary evaluation metric to identify the most effective model in predicting the likelihood of payment for parking tickets.
- Selected the best-performing model based on AUC-ROC scores, ensuring optimal predictive accuracy and reliability for informing decision-making processes within the government agency.

### Project 1

<!-- A large supermarket chain client -->

<!-- Created a Python data science POC for a large supermarket chain client

- Scraped neighborhood data from Wiki, acquired latitudes and longitudes data from the Geocoder, and retrieved location & venues data with Foursquare Places API.
- Used k-means clustering to categorize and provided the recommendations. -->

Developed a Python data science project tailored for a prominent supermarket chain client to optimize their store location strategy and enhance customer experience.

- Initiated the project by gathering comprehensive neighborhood data from Wikipedia, leveraging web scraping techniques to extract relevant information such as demographics, population density, and socioeconomic indicators.

- Augmented the neighborhood dataset by incorporating geographical coordinates using the Geocoder library, facilitating precise mapping and analysis of locations.

- Employed the powerful capabilities of the Foursquare Places API to retrieve detailed location and venue data, including nearby amenities, competitors, and popular attractions within each neighborhood.

- Utilized advanced data preprocessing techniques to clean and prepare the dataset for analysis, including handling missing values, normalizing features, and encoding categorical variables.

- Applied k-means clustering algorithm to segment neighborhoods based on similarities in venue characteristics, enabling the identification of distinct clusters representing different customer preferences and market segments.

- Conducted thorough exploratory data analysis (EDA) to gain insights into the distribution of clusters and understand the underlying patterns driving customer behavior and preferences.

- Leveraged visualization tools such as matplotlib and seaborn to create informative visualizations, including scatter plots, heatmaps, and cluster maps, to effectively communicate findings and insights to stakeholders.

- Provided actionable recommendations to the client based on the clustering results, including insights into optimal store locations, potential expansion opportunities, and strategies for improving customer engagement and retention.

## My AI Platforms

<!-- IBM Watson: IBM Watson is a comprehensive AI platform that offers a wide range of AI-powered services and tools for businesses and developers. It provides capabilities such as natural language processing (NLP), machine learning, computer vision, and speech recognition. Watson offers various APIs and services that enable developers to build and deploy AI-driven applications across different domains, including healthcare, finance, and customer service.

Microsoft Azure: Azure is a cloud computing platform by Microsoft that offers a diverse set of AI services and tools. Azure AI provides services such as Azure Machine Learning, Azure Cognitive Services, and Azure Bot Service, allowing developers to build intelligent applications with capabilities like speech recognition, language understanding, image recognition, and predictive analytics. Azure also offers GPU-accelerated computing instances for high-performance AI workloads. -->

### Amazon Web Services (AWS)

1. **Amazon SageMaker**: A fully managed machine learning service that enables developers to build, train, and deploy machine learning models at scale.
2. **Amazon Lex**: A service for building conversational interfaces into any application using voice and text.
3. **Amazon Polly**: A text-to-speech service that uses advanced deep learning technologies to synthesize speech that sounds like a human voice.
4. **Amazon Rekognition**: A service for adding image and video analysis to applications, including object and scene detection, facial analysis, and celebrity recognition.
5. **Amazon Comprehend**: A natural language processing (NLP) service for extracting insights and relationships from unstructured text.
6. **Amazon Transcribe**: A service for converting speech to text, enabling developers to transcribe audio files or streams into text in real time.
7. **AWS DeepLens**: A deep learning-enabled video camera that allows developers to experiment with and deploy deep learning models on the edge.
8. **AWS DeepRacer**: A fully autonomous 1/18th scale race car driven by reinforcement learning models, designed to help developers learn about reinforcement learning and machine learning in a fun way.
9. **AWS Deep Learning AMIs**: Preconfigured Amazon Machine Images (AMIs) that come with popular deep learning frameworks and libraries installed, allowing developers to quickly set up deep learning environments on EC2 instances.

### Google Cloud Platform (GCP)

1. **Google Cloud AI Platform**: A suite of machine learning services that enables developers to build, train, and deploy machine learning models at scale.
2. **Google Cloud AutoML**: A suite of machine learning products that enables developers with limited machine learning expertise to train high-quality custom models for specific use cases, such as image recognition and natural language processing.
3. **Google Cloud Vision API**: A service for integrating image recognition and analysis capabilities into applications.
4. **Google Cloud Speech-to-Text**: A service for converting speech into text in over 120 languages and variants.
5. **Google Cloud Text-to-Speech**: A service for converting text into natural-sounding speech in over 30 languages and variants.
6. **Google Cloud Translation API**: A service for translating text between languages using pre-trained models or custom machine learning models.
7. **Google Cloud Natural Language API**: A service for analyzing and extracting insights from unstructured text using machine learning models.
8. **Google Cloud Video Intelligence API**: A service for annotating and analyzing videos using machine learning models to detect objects, faces, and activities.
9. **Google Cloud Speech Command Recognition**: A service for recognizing speech commands in audio data using machine learning models, designed for IoT and other edge device applications.
10. **Google Cloud AI Hub**: A hosted repository and collaboration platform for discovering, sharing, and deploying machine learning models and datasets.

### Microsoft Azure

1. **Azure Machine Learning**: A cloud-based service for building, training, and deploying machine learning models at scale, with support for various programming languages and frameworks.
2. **Azure Cognitive Services**: A collection of AI-powered APIs and pre-built models for vision, speech, language, and decision-making, enabling developers to add intelligent features to their applications.
3. **Azure Bot Service**: A service for building, testing, and deploying chatbots across multiple channels, with built-in natural language understanding (NLU) capabilities.
4. **Azure Cognitive Search**: A fully managed search-as-a-service solution that uses AI to provide relevant search results and enrich content with semantic understanding.
5. **Azure Speech Service**: A cloud-based service for speech recognition and speech synthesis, with support for custom speech models and real-time transcription.
6. **Azure Computer Vision**: A service for analyzing and extracting insights from images and videos, including object detection, image recognition, and image classification.
7. **Azure Language Understanding (LUIS)**: A service for building natural language understanding models to interpret user intents and extract relevant information from text inputs.
8. **Azure Personalizer**: A reinforcement learning-based service for personalizing content and recommendations in applications, based on user interactions and feedback.
9. **Azure Video Analyzer**: A service for analyzing and extracting insights from videos, including object tracking, motion detection, and scene understanding.
10. **Azure IoT Edge**: A service for deploying AI models and analytics to edge devices, enabling real-time processing and decision-making at the edge of the network.

### IBM Watson

1. **Watson Assistant**: A conversational AI platform that enables developers to build and deploy chatbots and virtual assistants across multiple channels.
2. **Watson Discovery**: A service for analyzing unstructured data and extracting insights using natural language processing (NLP) and machine learning.
3. **Watson Language Translator**: A service for translating text between languages using advanced machine learning models.
4. **Watson Natural Language Understanding**: A service for analyzing text and extracting metadata such as entities, keywords, and sentiment.
5. **Watson Speech to Text**: A service for converting spoken language into text in real time, with support for multiple languages and audio formats.
6. **Watson Text to Speech**: A service for synthesizing natural-sounding speech from text in multiple languages and voices.
7. **Watson Visual Recognition**: A service for analyzing and classifying images using machine learning models, with support for custom image classifiers.
8. **Watson Studio**: A cloud-based platform for building and deploying machine learning models, with support for data preparation, model training, and model deployment.
9. **Watson Knowledge Catalog**: A service for managing and curating data assets, including datasets, models, and notebooks, to facilitate collaboration and reuse.
10. **Watson OpenScale**: A service for monitoring and managing AI models in production, including bias detection, model fairness, and performance monitoring.
